# Stock Analysis
Stockanalysis is a package that contains command line tools for downloading US End of Day stock data, as well as a pipeline for continuous training of machine learning models
AAAAAAAAAAA
## Install Package
The only requirement to install stockanalysis without GPU support is [Python 3](https://python.org). When installing stockanalysis, it is advisable to install the package in a Python virtual environment to keep the host Python environment clean. For more information refer to the Python article on [Virtual Environments](https://docs.python.org/3/tutorial/venv.html).  To install, clone the repository, and in the source directory run:
```
pip install .
```
### GPU support
The stockanalysis package uses [TensorFlow](https://www.tensorflow.org/) as its backend machine learning framework. To configure GPU support for stockanalysis requires configuring GPU support for TensorFlow. For details on how to configure GPU support for TensorFlow see: [GPU Support](https://www.tensorflow.org/install/gpu). After configuring TensorFlow's GPU support, clone the stockanalysis repository, and in the source directory run:
```
pip install .
```
## Install as Docker Image
To build the stockanalysis pipeline image, [Docker](https://docs.docker.com/install/) must first be installed on the host machine. Then in the source directory containing both `Dockerfile` and `.dockerignore` run:
```
docker build --tag [IMAGE NAME]:[VERSION NUMBER] .
```
### GPU support
Running the stockanalysis pipeline with GPU support is much easier using Docker than not. GPU support requires a CUDA®-enabled card, along with the appropriate [NVIDIA® GPU drivers](https://www.nvidia.com/drivers), and the [Nvidia Container Toolkit](https://github.com/NVIDIA/nvidia-docker/blob/master/README.md#quickstart) for Docker to be installed. After installing the required dependencies run:
```
docker build --build-arg GPU=1 --tag [IMAGE NAME]:[VERSION NUMBER] .
```
in the source directory containing both `Dockerfile` and `.dockerignore` to build the appropriate image with GPU support
## Deploying the Pipeline
If the stockanalysis package is installed locally, to start the current pipeline run:
```
stockanalysis run-pipeline [OPTIONS] PATH_TO_DATA PATH_TO_METADATA PATH_TO_MODELS
```
This command will start up the installed pipeline, pulling training data into `PATH_TO_DATA`, generating pipeline metadata in `PATH_TO_METADATA`, and storing trained models in `PATH_TO_MODELS`. To run the pipeline with GPU support include the option: `-g [GPU_MEMORY]` to specify the amount of GPU memory the training process has access to.
### Docker
To run the current pipeline as a Docker container run:
```
docker run --rm -v [PATH_TO_MODELS]:/models [IMAGE NAME]:[VERSION NUMBER] data meta models
```
The `-v` option bind mounts the `/models` file inside the container to host directory given by `PATH_TO_MODELS`.  To mount the other directories generated by the pipeline include other `-v` options following the same pattern.

For Docker images built with GPU support run:
```
docker run --rm --gpus all -v [PATH_TO_MODELS]:/models [IMAGE NAME]:[VERSION NUMBER] [GPU_MEMORY] data meta models
```
### Serving Models
The quickest way to serve models trained by the pipeline is by using [TensorFlow Serving with Docker](https://www.tensorflow.org/tfx/serving/docker). First pull the TensorFlow Serving docker image.
```
docker pull tensorflow/serving
```
To start the server run:
```
docker run -t --rm -p 8501:8501 -v [PATH_TO_MODELS]:/models -e MODEL_NAME=deployed_model tensorflow/serving &
```
The above command will serve models saved in host location: `[PATH_TO_MODELS]/deployed_model` using a REST API on host port 8501.

## Model Specs

|                |ASCII                          |HTML                         |
|----------------|-------------------------------|-----------------------------|
|Single backticks|`'Isn't this fun?'`            |'Isn't this fun?'            |
|Quotes          |`"Isn't this fun?"`            |"Isn't this fun?"            |
|Dashes          |`-- is en-dash, --- is em-dash`|-- is en-dash, --- is em-dash|
