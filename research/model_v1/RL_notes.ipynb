{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like any other Machine Learning/AI problem we split the universe up into the Agent, and the Environment. Where the agent interacts with the environment by choosing an action to do in the environment, and the environment interacts with the agent by giving the agent the necessary information to choose an action. All machine learning problems can be reduced to this setup.\n",
    "\n",
    "Formally we define this set up as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define action space $\\mathbb{A}$ as the set of all possible actions the agent can take in the environment.\n",
    "\n",
    "\n",
    "2. Define state $s$ as the collection of information necessary for the agent to intelligently (meaning with respect to achieving some goal {is this what intelligence means in humans, I don't know?}) choose an $a\\in\\mathbb{A}$\n",
    "\n",
    "\n",
    "3. Define state space $\\mathbb{S}$ as the set of all possible states.\n",
    "\n",
    "\n",
    "4. Define index space $T$ as the set of all possible moments in time when the agent could act. This defines the lifespan of the agent.\n",
    "\n",
    "\n",
    "5. Define the random action function/stochastic action process $\\{A_{t} | t \\in T\\}$ to model the potential sets of actions the agent can take throughout its lifespan. A sampled function $\\{a_{t}|a_{t} \\in \\mathbb{A} \\land t \\in T\\}$, defines a mapping between the moments in time and the action the agent took at that moment in time, and has a probability given by the stochastic process's probability density function.\n",
    "\n",
    "\n",
    "6. Define the random state function/stochastic state process $\\{S_{t}| t \\in T\\}$ to model the potential sets of states the agent could be in throughout its lifespan. A sampled function $\\{s_{t}|s_{t} \\in \\mathbb{S} \\land t \\in T\\}$, defines a mapping between the moments in time and the state the agent was in at that moment in time, and has a probability given by the stochastic process's probability density function.\n",
    "\n",
    "\n",
    "\n",
    "7. There exists model as: $A_{t} = f(S_{t}, \\epsilon_{t})$ where epsilon of t is an element of an unknown stochastic process that ensures A of t is still stochastic even when S of t is know.\n",
    "\n",
    "\n",
    "8. number 7. induces a optimal policy $\\pi$ which is the conditional probability density function of $A_{t}$ given $S_{t} = s_{t}$ if we sampled $a_{t}$ from our optimal policy we would be sampling $A_{t}$ correctly according to the underlying model.\n",
    "\n",
    "9. In order to achieve the goal, the agent should either choose actions sampled from the optimal policy, or choose the action which is the expected value of the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is 7-9 correct? 7-8 seems correct because if a perfect model exists describing how actions should be taken given the correct states, then this induces a conditional probability density function which is a policy. This policy should be optimal because if the model of the underlying phenomena that maximizes reward is correct, then the probability distribution that represents this model should be the policy that if acted to accordingly would produced the most reward. Not sure about 9. because if the model always chooses the expected value of the optimal stochastic policy, is this also an optimal policy???="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of Machine Learning is find the optimal policy/model $f$ for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Stochastic Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
