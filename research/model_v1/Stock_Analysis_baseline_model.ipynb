{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to train and evaluate a baseline model which will be used to gauge the future performance of our more complex models. We will define the baseline model to be the bare minimum model we can construct if we constrain our model to map any input to only one output value. Assuming our output data is distributed continuously, then the single value that best approximates the output distribution is the mean of the distribution. It follows naturally that the baseline model we will construct for the Wells Fargo stock price time series will be configured in such a way as to output the mean price of Wells Fargo stock across all of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Evaluating the Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the problem we are dealing with is a regression problem with only one target variable, we can represent any model that fits this problem description as: \n",
    "\n",
    "$$E(Y|X=X) = W \\cdot X + b$$\n",
    "\n",
    "In the case of simple linear regression, then $X$ directly represents the input features to the model, and $W$ and $b$ the weights and biases that the model learns. On the other hand if this equation represents a deep model architecture, then $W$ and $b$ would represent only the weights and biases that the model learns for the **output layer**.\n",
    "\n",
    "Assuming we have a model represented in the way as described above, we can morph it into something that behaves as our described baseline model by requiring $W = 0$ and $b = E(Y)$. This will force our model to output the same value of $E(Y)$ for every sample $X=x$ fed to it, which is precisely the behavior we require our baseline model to have. A corollary of this idea is that we can convert **any** model representable in this way to our baseline model by initializing the output layer's weights to 0 and bias to $E(Y)$.\n",
    "\n",
    "In the code below we will define and intialize a baseline model using the train dataset we pickled to disk in `Stock_Analysis_EDA.ipynb`, and then evaluate the model on the validation dataset pickled to disk in `Stock_Analysis_EDA.ipynb`. We will plan on evaluating the model's mean squared error as well as its accuracy in being able to predict daily stock trends. We chose mean squared error because it will be used as our loss function to minimize when training more complex models, while the accuracy metric was chosen simply because it will be interesting to learn how well our models perform at predicting trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel in order to fully clear GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Configuring Virtual GPU and Loading Data\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from stockanalysis.train import config_hardware\n",
    "\n",
    "# Project Paths\n",
    "project_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "path_to_data = os.path.join(project_dir, 'data')\n",
    "\n",
    "# Random Seed\n",
    "seed = None\n",
    "\n",
    "# Configuring GPU and TensorFlow\n",
    "config_hardware(gpu_memory=7000, seed=seed)\n",
    "\n",
    "# Loading Train Dataset\n",
    "with open(os.path.join(path_to_data, 'train_dataset.pickle'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "# Loading Test Dataset\n",
    "with open(os.path.join(path_to_data, 'val_dataset.pickle'), 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "    \n",
    "# Loading Train Dataset's Vocabulary\n",
    "with open(os.path.join(path_to_data, 'vocab_8k_norm_train_WFC_JPM_BAC_C.json'), 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def baseline_model(output_bias_init=None):\n",
    "    \n",
    "    if output_bias_init is not None:\n",
    "        output_bias = output_bias_init['adjusted_close_target_WFC']\n",
    "        output_bias_init = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    inputs = {\n",
    "              'adjusted_close_WFC': tf.keras.Input(shape=(5,), name='adjusted_close_WFC', dtype=tf.float32),\n",
    "              '8-k_WFC': tf.keras.Input(shape=(None,), name='8-k_WFC', dtype=tf.int64),\n",
    "              'adjusted_close_JPM': tf.keras.Input(shape=(5,), name='adjusted_close_JPM', dtype=tf.float32),\n",
    "              '8-k_JPM': tf.keras.Input(shape=(None,), name='8-k_JPM', dtype=tf.int64),\n",
    "              'adjusted_close_BAC': tf.keras.Input(shape=(5,), name='adjusted_close_BAC', dtype=tf.float32),\n",
    "              '8-k_BAC': tf.keras.Input(shape=(None,), name='8-k_BAC', dtype=tf.int64),\n",
    "              'adjusted_close_C': tf.keras.Input(shape=(5,), name='adjusted_close_C', dtype=tf.float32),\n",
    "              '8-k_C': tf.keras.Input(shape=(None,), name='8-k_C', dtype=tf.int64)\n",
    "             }\n",
    "    \n",
    "    features = tf.keras.layers.Concatenate()([inputs[fname] for fname in inputs.keys() if '8-k' not in fname])\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, \n",
    "                                         name='adjusted_close_target_WFC')\n",
    "    \n",
    "    outputs = {\n",
    "               'adjusted_close_target_WFC': output_layer(features)\n",
    "              }\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='baseline_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting both our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names and shapes for Training Data:\n",
      "adjusted_close_WFC: (5,)\n",
      "8-k_WFC: (1562,)\n",
      "adjusted_close_JPM: (5,)\n",
      "8-k_JPM: (90363,)\n",
      "adjusted_close_BAC: (5,)\n",
      "8-k_BAC: (15826,)\n",
      "adjusted_close_C: (5,)\n",
      "8-k_C: (53958,)\n",
      "\n",
      "Feature names and shapes for Validation Data:\n",
      "adjusted_close_WFC: (5,)\n",
      "8-k_WFC: (1364,)\n",
      "adjusted_close_JPM: (5,)\n",
      "8-k_JPM: (1706,)\n",
      "adjusted_close_BAC: (5,)\n",
      "8-k_BAC: (2026,)\n",
      "adjusted_close_C: (5,)\n",
      "8-k_C: (2636,)\n",
      "\n",
      "Train set size: 3014\n",
      "Validation set size: 1001\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train_dataset\n",
    "X_val, y_val = val_dataset\n",
    "\n",
    "print('Feature names and shapes for Training Data:')\n",
    "for key in X_train:\n",
    "    print('{}: {}'.format(key, X_train[key].shape[1:]))\n",
    "print()\n",
    "print('Feature names and shapes for Validation Data:')\n",
    "for key in X_val:\n",
    "    print('{}: {}'.format(key, X_val[key].shape[1:]))\n",
    "print()\n",
    "print('Train set size: {}'.format(len(y_train['adjusted_close_target_WFC'])))\n",
    "print('Validation set size: {}'.format(len(y_val['adjusted_close_target_WFC'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from stockanalysis.train import build_compiled_model\n",
    "\n",
    "# Defining Hyperparameters\n",
    "output_bias_init = {key: y_train[key].mean() for key in y_train}\n",
    "model_params = {'output_bias_init': output_bias_init}\n",
    "training_params = {'batch_size': None, 'epochs': None}\n",
    "loss = tf.keras.losses.MeanSquaredError\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "optimizer_params = {}\n",
    "\n",
    "model_version = None\n",
    "hyperparameters = {\n",
    "                   'model_parameters': model_params,\n",
    "                   'training_parameters': training_params,\n",
    "                   'loss': loss, \n",
    "                   'optimizer': optimizer, \n",
    "                   'optimizer_parameters': optimizer_params, \n",
    "                   'version': model_version\n",
    "                  }\n",
    "\n",
    "# Defining Metrics\n",
    "metrics = []\n",
    "\n",
    "# Setting unique Run Number \n",
    "run_number = None\n",
    "\n",
    "model_baseline, initial_epoch = build_compiled_model(baseline_model, hyperparameters, metrics, run_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "metrics_train = model_baseline.evaluate(X_train, y_train, verbose=0)\n",
    "metrics_val = model_baseline.evaluate(X_val, y_val, verbose=0)\n",
    "m_preds_train = model_baseline.predict(X_train)\n",
    "m_preds_val = model_baseline.predict(X_val)\n",
    "m_preds_up_train = ((m_preds_train[1:, 0] - y_train['adjusted_close_target_WFC'][:-1]) > 0).astype(int)\n",
    "m_preds_up_val = ((m_preds_val[1:, 0] - y_val['adjusted_close_target_WFC'][:-1]) > 0).astype(int)\n",
    "labels_up_train = ((y_train['adjusted_close_target_WFC'][1:] - y_train['adjusted_close_target_WFC'][:-1]) > 0).astype(int)\n",
    "labels_up_val = ((y_val['adjusted_close_target_WFC'][1:] - y_val['adjusted_close_target_WFC'][:-1]) > 0).astype(int)\n",
    "up_cls_acc_train = np.mean(np.equal(m_preds_up_train, labels_up_train))\n",
    "up_cls_acc_val = np.mean(np.equal(m_preds_up_val, labels_up_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on train dataset for Baseline Model: 18.943944523487964\n",
      "Loss on validation dataset for Baseline Model: 387.5771874609765\n",
      "\n",
      "Metrics for Classifying Upward Movements\n",
      "Accuracy on train dataset for Baseline Model: 0.7467640225688682\n",
      "Accuracy on validation dataset for Baseline Model: 0.507\n"
     ]
    }
   ],
   "source": [
    "assert all((pred == m_preds_train[0]) for pred in m_preds_train)\n",
    "\n",
    "print('Loss on train dataset for Baseline Model: {}'.format(metrics_train))\n",
    "print('Loss on validation dataset for Baseline Model: {}'.format(metrics_val))\n",
    "print()\n",
    "print('Metrics for Classifying Upward Movements')\n",
    "print('Accuracy on train dataset for Baseline Model: {}'.format(up_cls_acc_train))\n",
    "print('Accuracy on validation dataset for Baseline Model: {}'.format(up_cls_acc_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
