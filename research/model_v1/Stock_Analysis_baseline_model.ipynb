{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is the bare minimum model we can manually construct from just our Exploratory Data Analysis. This model will give us baselines that hopefully our trained models will beat. Since the problem we are dealing with is a regression problem with only one target variable, we can represent any model that fits this problem description as: \n",
    "\n",
    "$$E(Y|X=X) = W \\cdot X + b$$\n",
    "\n",
    "In the case of simple linear regression then $X$ directly represents the input features to the model, and $W$ and $b$ the weights and biases that the model learns. On the other hand if this equation represents a deep model architecture, then $W$ and $b$ would represent only the weights and biases that the model learns for the **output layer**. This implies that $X$ would be a function of the input features to the model and would represent the input passed to the output layer of the model, or in other words $X$ represents the features the previous layers of the model **learned** to extract from the given original input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express our baseline model as one that predicts the expected value of the target variable no matter what the predictor variable happens to be. If we assume that there exists a relation between our predictor variables and target variable, and we average across all values of our predictor variables, then we find that any trained model of the relationship between predictor and target variables should on average predict the expected value of the target variable. This means that the bare minimum we can ask a model is that on average it predicts the expected value of the target variable. This implies that if we were to design a model that predicts the target variable without taking the predictor variable into account, then inorder to satisfy the bare minimum constraint, this model would need to predict the mean of our target variable. Mathematically we can express or baseline model as:\n",
    "\n",
    "$E(Y|X=X) = E(Y) = \\mu = W \\cdot X + b$ such that $W = 0$ in order to satisfy the input independent constraint.\n",
    "\n",
    "This implies $b = \\mu$.\n",
    "\n",
    "This also implies that we can convert any model to a model equivalent to the baseline model, as long as we initialize its output layers weights to zero, and its bias to the expected value of our target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our specific case we know from our exploratory data analysis that the means of all our target variables, logarithmic adjusted returns, are close to zero for each stock. This is also supported by theory, which predicts that the logarthimic return on a stock should follow a normal distribution centered around zero (or at least very closely conform to a normal distribution). This means to build our baseline model we must create a linear regression model where both the weights and bias are initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel in order to fully clear GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n",
      "Visible GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries and Configuring virtual GPU\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from stockanalysis.train import config_hardware\n",
    "\n",
    "# Project Paths\n",
    "project_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "path_to_data = os.path.join(project_dir, 'data')\n",
    "path_to_docs = os.path.join(path_to_data, 'documents')\n",
    "path_to_models = os.path.join(project_dir, 'models')\n",
    "\n",
    "# Random Seed\n",
    "seed = 42\n",
    "\n",
    "# Configuring GPU and Random States\n",
    "config_hardware(gpu_memory=None, seed=seed)\n",
    "\n",
    "\n",
    "# Loading Train Dataset\n",
    "with open(os.path.join(path_to_data, 'train_dataset.pickle'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "# Loading Test Dataset\n",
    "with open(os.path.join(path_to_data, 'val_dataset.pickle'), 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "# Defining Helper Functions    \n",
    "def build_compiled_model(build_model, hparams, loss, optimizer, metrics, callbacks=None):\n",
    "    model = build_model(**hparams)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics, callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "# Model Agnostic Parameters\n",
    "LOSS = tf.keras.losses.MeanSquaredError()\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()\n",
    "METRICS = []\n",
    "CALLBACKS = None\n",
    "with open(os.path.join(path_to_data, 'vocab_8k_norm_train_WFC_JPM_BAC_C.json'), 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(output_bias_init):\n",
    "    if output_bias_init is not None:\n",
    "        output_bias_init = tf.keras.initializers.Constant(output_bias_init)\n",
    "        \n",
    "    inputs = {'log_adj_daily_returns': tf.keras.Input(shape=(5,), name='log_adj_daily_returns', dtype=tf.float32),\n",
    "              '8-k': tf.keras.Input(shape=(None,), name='8-k', dtype=tf.int64)}\n",
    "    output_layer = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, \n",
    "                                         name='log_adj_daily_returns_target')\n",
    "    outputs = {'log_adj_daily_returns_target': output_layer(inputs['log_adj_daily_returns'])}\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='baseline_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_bias_init = 0\n",
    "hparams = {'output_bias_init': output_bias_init}\n",
    "\n",
    "baseline_m = build_compiled_model(baseline_model, hparams, loss=LOSS,\n",
    "                                  optimizer=OPTIMIZER, metrics=METRICS, callbacks=CALLBACKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_dataset\n",
    "X_val, y_val = val_dataset\n",
    "\n",
    "baseline_predictions = baseline_m.predict(X_train)\n",
    "baseline_results_train = baseline_m.evaluate(X_train, y_train, verbose=0)\n",
    "baseline_results_val = baseline_m.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss for Baseline Model: 0.0011974310440253082\n",
      "Test Loss for Baseline Model: 0.00020920674223289128\n"
     ]
    }
   ],
   "source": [
    "assert all((pred == baseline_predictions[0]) for pred in baseline_predictions)\n",
    "\n",
    "print('Train Loss for Baseline Model: {}'.format(baseline_results_train))\n",
    "print('Test Loss for Baseline Model: {}'.format(baseline_results_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n",
      "Visible GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries and Configuring virtual GPU\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from stockanalysis.train import config_hardware\n",
    "\n",
    "# Project Paths\n",
    "project_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "path_to_data = os.path.join(project_dir, 'data')\n",
    "path_to_docs = os.path.join(path_to_data, 'documents')\n",
    "path_to_models = os.path.join(project_dir, 'models')\n",
    "\n",
    "# Random Seed\n",
    "seed = 42\n",
    "\n",
    "# Configuring GPU and Random States\n",
    "config_hardware(gpu_memory=None, seed=seed)\n",
    "\n",
    "\n",
    "# Loading Train Dataset\n",
    "with open(os.path.join(path_to_data, 'train_datasetp2.pickle'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "# Loading Test Dataset\n",
    "with open(os.path.join(path_to_data, 'val_datasetp2.pickle'), 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "\n",
    "# Defining Helper Functions    \n",
    "def build_compiled_model(build_model, hparams, loss, loss_weights, optimizer, metrics, callbacks=None):\n",
    "    model = build_model(**hparams)\n",
    "    model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer, metrics=metrics, callbacks=callbacks)\n",
    "    return model\n",
    "\n",
    "# Model Agnostic Parameters\n",
    "LOSS = {\n",
    "        'log_adj_daily_returns_target_WFC': tf.keras.losses.MeanSquaredError(),\n",
    "       }\n",
    "\n",
    "OPTIMIZER = tf.keras.optimizers.Adam()\n",
    "\n",
    "CALLBACKS = None\n",
    "with open(os.path.join(path_to_data, 'vocab_8k_norm_trainp2_WFC_JPM_BAC_C.json'), 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['log_adj_daily_returns_WFC', '8-k_WFC', 'log_adj_daily_returns_JPM', '8-k_JPM', 'log_adj_daily_returns_BAC', '8-k_BAC', 'log_adj_daily_returns_C', '8-k_C'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_print_model_stats(model, path, model_name):\n",
    "    m = model()\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fname = os.path.join(path, model_name)\n",
    "    tf.keras.utils.plot_model(m, fname + '.png', show_shapes=True, expand_nested=True)\n",
    "    tf.keras.backend.clear_session()\n",
    "    return m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"baseline_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "log_adj_daily_returns_WFC (Inpu [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_JPM (Inpu [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_BAC (Inpu [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_C (InputL [(None, 5)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20)           0           log_adj_daily_returns_WFC[0][0]  \n",
      "                                                                 log_adj_daily_returns_JPM[0][0]  \n",
      "                                                                 log_adj_daily_returns_BAC[0][0]  \n",
      "                                                                 log_adj_daily_returns_C[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "8-k_BAC (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "8-k_C (InputLayer)              [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "8-k_JPM (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "8-k_WFC (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_target_BA (None, 1)            21          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_target_C  (None, 1)            21          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_target_JP (None, 1)            21          concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "log_adj_daily_returns_target_WF (None, 1)            21          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 84\n",
      "Trainable params: 84\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def baseline_model(output_bias_init=0):\n",
    "    \n",
    "    if output_bias_init is not None:\n",
    "        output_bias_init = tf.keras.initializers.Constant(output_bias_init)\n",
    "        \n",
    "    inputs = {\n",
    "              'log_adj_daily_returns_WFC': tf.keras.Input(shape=(5,), name='log_adj_daily_returns_WFC', dtype=tf.float32),\n",
    "              '8-k_WFC': tf.keras.Input(shape=(None,), name='8-k_WFC', dtype=tf.int64),\n",
    "              'log_adj_daily_returns_JPM': tf.keras.Input(shape=(5,), name='log_adj_daily_returns_JPM', dtype=tf.float32),\n",
    "              '8-k_JPM': tf.keras.Input(shape=(None,), name='8-k_JPM', dtype=tf.int64),\n",
    "              'log_adj_daily_returns_BAC': tf.keras.Input(shape=(5,), name='log_adj_daily_returns_BAC', dtype=tf.float32),\n",
    "              '8-k_BAC': tf.keras.Input(shape=(None,), name='8-k_BAC', dtype=tf.int64),\n",
    "              'log_adj_daily_returns_C': tf.keras.Input(shape=(5,), name='log_adj_daily_returns_C', dtype=tf.float32),\n",
    "              '8-k_C': tf.keras.Input(shape=(None,), name='8-k_C', dtype=tf.int64)\n",
    "             }\n",
    "    \n",
    "    features = tf.keras.layers.Concatenate()([inputs[fname] for fname in inputs.keys() if '8-k' not in fname])\n",
    "    \n",
    "    output_wfc = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, name='log_adj_daily_returns_target_WFC')(features)\n",
    "    output_jpm = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, name='log_adj_daily_returns_target_JPM')(features)\n",
    "    output_bac = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, name='log_adj_daily_returns_target_BAC')(features)\n",
    "    output_c = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, name='log_adj_daily_returns_target_C')(features)\n",
    "    \n",
    "    outputs = {\n",
    "               'log_adj_daily_returns_target_WFC': output_wfc, \n",
    "               'log_adj_daily_returns_target_JPM': output_jpm,\n",
    "               'log_adj_daily_returns_target_BAC': output_bac,\n",
    "               'log_adj_daily_returns_target_C': output_c\n",
    "              }\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='baseline_model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "gen_print_model_stats(baseline_model, 'pics', 'testbaseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](pics/testbaseline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output log_adj_daily_returns_target_BAC missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to log_adj_daily_returns_target_BAC.\n",
      "WARNING:tensorflow:Output log_adj_daily_returns_target_C missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to log_adj_daily_returns_target_C.\n",
      "WARNING:tensorflow:Output log_adj_daily_returns_target_JPM missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to log_adj_daily_returns_target_JPM.\n"
     ]
    }
   ],
   "source": [
    "output_bias_init = 0\n",
    "hparams = {'output_bias_init': output_bias_init}\n",
    "\n",
    "baseline_m = build_compiled_model(baseline_model, hparams, loss=LOSS, loss_weights=None, \n",
    "                                  optimizer=OPTIMIZER, metrics=None, callbacks=CALLBACKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_dataset\n",
    "X_val, y_val = val_dataset\n",
    "\n",
    "baseline_predictions = baseline_m.predict(X_train)\n",
    "baseline_results_train = baseline_m.evaluate(X_train, y_train, verbose=0)\n",
    "baseline_results_val = baseline_m.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Names of losses: ['loss', 'log_adj_daily_returns_target_WFC_loss']\n",
      "\n",
      "Train Loss for Baseline Model: [0.0008051489573671401, 0.00079874013]\n",
      "Test Loss for Baseline Model: [0.00015122863568831234, 0.00014940402]\n"
     ]
    }
   ],
   "source": [
    "#assert all(all(pred[:, 0] == pred[0, 0]) for pred in baseline_predictions)\n",
    "\n",
    "print('Names of losses: {}\\n'.format(baseline_m.metrics_names))\n",
    "print('Train Loss for Baseline Model: {}'.format(baseline_results_train))\n",
    "print('Test Loss for Baseline Model: {}'.format(baseline_results_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004108154098503292"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(baseline_results_train[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010604808485368267"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(baseline_results_val[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
