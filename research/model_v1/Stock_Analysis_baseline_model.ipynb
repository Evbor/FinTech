{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is the bare minimum model we can manually construct from just our Exploratory Data Analysis. This model will give us baselines that hopefully our trained models will beat. Since the problem we are dealing with is a regression problem with only one target variable, we can represent any model that fits this problem description as: \n",
    "\n",
    "$$E(Y|X=X) = W \\cdot X + b$$\n",
    "\n",
    "In the case of simple linear regression then $X$ directly represents the input features to the model, and $W$ and $b$ the weights and biases that the model learns. On the other hand if this equation represents a deep model architecture, then $W$ and $b$ would represent only the weights and biases that the model learns for the **output layer**. This implies that $X$ would be a function of the input features to the model and would represent the input passed to the output layer of the model, or in other words $X$ represents the features the previous layers of the model **learned** to extract from the given original input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express our baseline model as one that predicts the expected value of the target variable no matter what the predictor variable happens to be. If we assume that there exists a relation between our predictor variables and target variable, and we average across all values of our predictor variables, then we find that any trained model of the relationship between predictor and target variables should on average predict the expected value of the target variable. This means that the bare minimum we can ask a model is that on average it predicts the expected value of the target variable. This implies that if we were to design a model that predicts the target variable without taking the predictor variable into account, then inorder to satisfy the bare minimum constraint, this model would need to predict the mean of our target variable. Mathematically we can express or baseline model as:\n",
    "\n",
    "$E(Y|X=X) = E(Y) = \\mu = W \\cdot X + b$ such that $W = 0$ in order to satisfy the input independent constraint.\n",
    "\n",
    "This implies $b = \\mu$.\n",
    "\n",
    "This also implies that we can convert any model to a model equivalent to the baseline model, as long as we initialize its output layers weights to zero, and its bias to the expected value of our target variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our specific case we know from our exploratory data analysis that the means of all our target variables, logarithmic adjusted returns, are close to zero for each stock. This is also supported by theory, which predicts that the logarthimic return on a stock should follow a normal distribution centered around zero (or at least very closely conform to a normal distribution). This means to build our baseline model we must create a linear regression model where both the weights and bias are initialized to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart kernel in order to fully clear GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "# Configuring Virtual GPU and Loading Data\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from stockanalysis.train import config_hardware\n",
    "\n",
    "# Project Paths\n",
    "project_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "path_to_data = os.path.join(project_dir, 'data')\n",
    "\n",
    "# Random Seed\n",
    "seed = None\n",
    "\n",
    "# Configuring GPU and TensorFlow\n",
    "config_hardware(gpu_memory=7000, seed=seed)\n",
    "\n",
    "# Loading Train Dataset\n",
    "with open(os.path.join(path_to_data, 'train_datasetp2.pickle'), 'rb') as f:\n",
    "    train_dataset = pickle.load(f)\n",
    "\n",
    "# Loading Test Dataset\n",
    "with open(os.path.join(path_to_data, 'val_datasetp2.pickle'), 'rb') as f:\n",
    "    val_dataset = pickle.load(f)\n",
    "    \n",
    "# Loading Train Dataset's Vocabulary\n",
    "with open(os.path.join(path_to_data, 'vocab_8k_norm_trainp2_WFC_JPM_BAC_C.json'), 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def baseline_model(output_bias_init=None):\n",
    "    \n",
    "    if output_bias_init is not None:\n",
    "        output_bias = output_bias_init['adjusted_close_target_WFC']\n",
    "        output_bias_init = tf.keras.initializers.Constant(output_bias)\n",
    "        \n",
    "    inputs = {\n",
    "              'adjusted_close_WFC': tf.keras.Input(shape=(5,), name='adjusted_close_WFC', dtype=tf.float32),\n",
    "              '8-k_WFC': tf.keras.Input(shape=(None,), name='8-k_WFC', dtype=tf.int64),\n",
    "              'adjusted_close_JPM': tf.keras.Input(shape=(5,), name='adjusted_close_JPM', dtype=tf.float32),\n",
    "              '8-k_JPM': tf.keras.Input(shape=(None,), name='8-k_JPM', dtype=tf.int64),\n",
    "              'adjusted_close_BAC': tf.keras.Input(shape=(5,), name='adjusted_close_BAC', dtype=tf.float32),\n",
    "              '8-k_BAC': tf.keras.Input(shape=(None,), name='8-k_BAC', dtype=tf.int64),\n",
    "              'adjusted_close_C': tf.keras.Input(shape=(5,), name='adjusted_close_C', dtype=tf.float32),\n",
    "              '8-k_C': tf.keras.Input(shape=(None,), name='8-k_C', dtype=tf.int64)\n",
    "             }\n",
    "    \n",
    "    features = tf.keras.layers.Concatenate()([inputs[fname] for fname in inputs.keys() if '8-k' not in fname])\n",
    "    \n",
    "    output_layer = tf.keras.layers.Dense(1, kernel_initializer='zeros', bias_initializer=output_bias_init, \n",
    "                                         name='adjusted_close_target_WFC')\n",
    "    \n",
    "    outputs = {\n",
    "               'adjusted_close_target_WFC': output_layer(features)\n",
    "              }\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='baseline_model')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting both our training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names and shapes for Training Data:\n",
      "adjusted_close_WFC: (5,)\n",
      "8-k_WFC: (1562,)\n",
      "adjusted_close_JPM: (5,)\n",
      "8-k_JPM: (90363,)\n",
      "adjusted_close_BAC: (5,)\n",
      "8-k_BAC: (15826,)\n",
      "adjusted_close_C: (5,)\n",
      "8-k_C: (53958,)\n",
      "\n",
      "Feature names and shapes for Validation Data:\n",
      "adjusted_close_WFC: (5,)\n",
      "8-k_WFC: (1364,)\n",
      "adjusted_close_JPM: (5,)\n",
      "8-k_JPM: (1706,)\n",
      "adjusted_close_BAC: (5,)\n",
      "8-k_BAC: (2026,)\n",
      "adjusted_close_C: (5,)\n",
      "8-k_C: (2636,)\n",
      "\n",
      "Train set size: 3014\n",
      "Validation set size: 1001\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = train_dataset\n",
    "X_val, y_val = val_dataset\n",
    "\n",
    "print('Feature names and shapes for Training Data:')\n",
    "for key in X_train:\n",
    "    print('{}: {}'.format(key, X_train[key].shape[1:]))\n",
    "print()\n",
    "print('Feature names and shapes for Validation Data:')\n",
    "for key in X_val:\n",
    "    print('{}: {}'.format(key, X_val[key].shape[1:]))\n",
    "print()\n",
    "print('Train set size: {}'.format(len(y_train['adjusted_close_target_WFC'])))\n",
    "print('Validation set size: {}'.format(len(y_val['adjusted_close_target_WFC'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from stockanalysis.train import build_compiled_model\n",
    "\n",
    "# Defining Hyperparameters\n",
    "output_bias_init = {key: y_train[key].mean() for key in y_train}\n",
    "model_params = {'output_bias_init': output_bias_init}\n",
    "training_params = {'batch_size': None, 'epochs': None}\n",
    "loss = tf.keras.losses.MeanSquaredError\n",
    "optimizer = tf.keras.optimizers.Adam\n",
    "optimizer_params = {}\n",
    "\n",
    "model_version = None\n",
    "hyperparameters = {\n",
    "                   'model_parameters': model_params,\n",
    "                   'training_parameters': training_params,\n",
    "                   'loss': loss, \n",
    "                   'optimizer': optimizer, \n",
    "                   'optimizer_parameters': optimizer_params, \n",
    "                   'version': model_version\n",
    "                  }\n",
    "\n",
    "# Defining Metrics\n",
    "metrics = []\n",
    "\n",
    "# Setting unique Run Number \n",
    "run_number = None\n",
    "\n",
    "model_baseline, initial_epoch = build_compiled_model(baseline_model, hyperparameters, metrics, run_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predictions = model_baseline.predict(X_train)\n",
    "baseline_results_train = model_baseline.evaluate(X_train, y_train, verbose=0)\n",
    "baseline_results_val = model_baseline.evaluate(X_val, y_val, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss for Baseline Model: 18.943944523487964\n",
      "Test Loss for Baseline Model: 387.5771874609765\n"
     ]
    }
   ],
   "source": [
    "assert all((pred == baseline_predictions[0]) for pred in baseline_predictions)\n",
    "\n",
    "print('Train Loss for Baseline Model: {}'.format(baseline_results_train))\n",
    "print('Test Loss for Baseline Model: {}'.format(baseline_results_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAlcualtte up predictions and down predictions for baseline then pretty up and rerun all epxerimentes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
