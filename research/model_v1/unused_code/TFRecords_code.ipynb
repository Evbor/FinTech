{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the tf dataset section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains snippets of code I might want to refer back to in the future, te topic is tf dataset operations and tfrecords functions for tensorflow 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions and classes used to load the dataset from its TFRecord file\n",
    "\n",
    "def parse_example(example_proto, feature_description):\n",
    "    '''\n",
    "    Parses example proto from\n",
    "    \n",
    "    :param example_proto: \n",
    "    :param feature_description: \n",
    "    '''\n",
    "    \n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Reconstructing Ragged Tensors from Example\n",
    "    for t in tickers:\n",
    "        example['_'.join(['docs', t])] = tf.RaggedTensor.from_row_lengths(example['docs_{}/vals'.format(t)].values,\n",
    "                                                           row_lengths=example['docs_{}/lens'.format(t)].values)\n",
    "\n",
    "    # Deleting Redundant Keys\n",
    "    for t in tickers:\n",
    "        del example['docs_{}/vals'.format(t)]\n",
    "        del example['docs_{}/lens'.format(t)]\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "# Loading the raw dataset from the TFRecord file\n",
    "dataset = tf.data.TFRecordDataset(os.path.join(path_to_data, 'dataset.tfrecord'))\n",
    "# Loading the dataset's feature_description\n",
    "with open(os.path.join(path_to_data, 'dataset_feature_description.pickle'), 'rb') as f:\n",
    "    feature_description = pickle.load(f)\n",
    "# Decoding the raw dataset using the dataset's feature_description\n",
    "dataset = dataset.map(lambda example_proto: parse_example(example_proto, feature_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spliting the dataset by stock ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(example, features, ticker):\n",
    "    return {feature_name: example['_'.join([feature_name, ticker])] for feature_name in features}\n",
    "\n",
    "datasets = [dataset.map(lambda ex: split(ex, ['log_adj_daily_returns', 'docs'], t)) for t in tickers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reshaping datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions and classes used to reshape datasets\n",
    "\n",
    "def make_window_dataset(ds, window_size, shift=1, stride=1):\n",
    "    \n",
    "    windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "    \n",
    "    feature_datasets = {key: windows.flat_map(lambda x: x[key].batch(window_size, drop_remainder=True))\n",
    "                        for key in windows.element_spec.keys()}\n",
    "    \n",
    "    return tf.data.Dataset.zip(feature_datasets)\n",
    "\n",
    "def extract_labels(timeslice, label_features):\n",
    "    labels = {}\n",
    "    \n",
    "    for feature_key in timeslice.keys():\n",
    "        feature_timeslice = timeslice[feature_key]\n",
    "        if feature_key in label_features:\n",
    "            labels[feature_key] = feature_timeslice[-1]\n",
    "        timeslice[feature_key] = feature_timeslice[:-1]\n",
    "        \n",
    "    return (timeslice, labels)\n",
    "\n",
    "\n",
    "def to_time_series(ds, label_features, window_size, steps_to_pred=1, num_of_preds=1):\n",
    "    \n",
    "    # making full time series Dataset object (features + labels)\n",
    "    full_ts_ds = make_window_dataset(ds, window_size=window_size+1)\n",
    "    \n",
    "    # mapping dataset to Dataset where each el is: (features: dict, labels)\n",
    "    ts_ds = full_ts_ds.map(lambda s: extract_labels(s, label_features))\n",
    "    \n",
    "    return ts_ds\n",
    "\n",
    "def sample_documents(sample):\n",
    "    # Extracting all documents in the sample\n",
    "    docs_in_sample = sample.values\n",
    "    # Sampling a random document from all the documents in the sample\n",
    "    if docs_in_sample.nrows() != 0:\n",
    "        i = tf.random.uniform([1], maxval=docs_in_sample.nrows(), dtype=tf.int64)[0]\n",
    "        sample_doc = docs_in_sample[i]\n",
    "    else:\n",
    "        sample_doc = tf.constant([], dtype=tf.int64)\n",
    "        \n",
    "    return sample_doc\n",
    "\n",
    "def select_doc(features, labels):\n",
    "    \n",
    "    for fname in features.keys():\n",
    "        feature = features[fname]\n",
    "        timesteps = feature.shape[0]\n",
    "        # Feature is a doc feature\n",
    "        if isinstance(feature, tf.RaggedTensor):\n",
    "            doc = sample_documents(feature)\n",
    "            feature = tf.stack([doc for day in range(timesteps)])\n",
    "            features[fname] = feature\n",
    "        \n",
    "    return (features, *list(labels.values()))\n",
    "\n",
    "def filter_fn(f, l):\n",
    "    shape = tf.shape(f['docs'])[1]\n",
    "    return tf.math.not_equal(shape, 0)\n",
    "\n",
    "def reshape(dataset, window_size, label_name):\n",
    "    # Converting to time series\n",
    "    ds = to_time_series(dataset, label_name, window_size=window_size)\n",
    "    # Selecting document features\n",
    "    ds = ds.map(select_doc)\n",
    "    # Filtering out elements without a document feature\n",
    "    ds = ds.filter(filter_fn)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping Datasets\n",
    "reshaped_datasets = list(map(lambda d: reshape(d, TIMESTEPS, 'log_adj_daily_returns'), datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "4. Concatenating datasets, and shuffling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reduce(lambda a, b: a.concatenate(b), reshaped_datasets).shuffle(1000, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Splitting dataset into train, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Functions and Classes for splitting datasets into train, validation, and test datasets\n",
    "\n",
    "def k_folds(dataset, k):\n",
    "    '''\n",
    "    Splits :param dataset: into :param k: number of equally sized (or close to equally sized) components.\n",
    "    \n",
    "    :param dataset: tf.data.Dataset, dataset to split into k folds\n",
    "    :param k: int, number of folds to split :param dataset: into\n",
    "    \n",
    "    ---> list, of tf.data.Dataset objets\n",
    "    '''\n",
    "    return [dataset.shard(k, i) for i in range(k)]\n",
    "\n",
    "def train_test_split(dataset, train_size):\n",
    "    '''\n",
    "    Splits :param dataset: into\n",
    "    \n",
    "    :param dataset: tf.data.Dataset, to split into train and test datasets\n",
    "    :param train_size: float between 0 and 1, proportion of :param dataset: to put into train dataset\n",
    "    \n",
    "    ---> (tf.data.Dataset, tf.data.Dataset), representing train, test datasets\n",
    "    '''\n",
    "    train_size = Fraction(train_size).limit_denominator()\n",
    "    x, k = train_size.numerator, train_size.denominator\n",
    "    folds = k_folds(dataset, k)\n",
    "    train = reduce(lambda a, b: a.concatenate(b), folds[:x])\n",
    "    test = reduce(lambda a, b: a.concatenate(b), folds[x:])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our models we will reserve 60% of the dataset for training, 20% for validation, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset into train, validation, test datasets\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset, test_val_dataset = train_test_split(dataset, train_size=0.6)\n",
    "val_dataset, test_dataset = train_test_split(test_val_dataset, train_size=0.5)\n",
    "\n",
    "# Prepping datasets for modeling\n",
    "train_dataset = (train_dataset.shuffle(10)\n",
    "                 .padded_batch(BATCH_SIZE, \n",
    "                               padded_shapes=({'log_adj_daily_returns': [TIMESTEPS,], \n",
    "                                               'docs': [TIMESTEPS, None]}, [])))\n",
    "val_dataset = (val_dataset.shuffle(10)\n",
    "               .padded_batch(BATCH_SIZE,\n",
    "                             padded_shapes=({'log_adj_daily_returns': [TIMESTEPS,], \n",
    "                                             'docs': [TIMESTEPS, None]}, [])))\n",
    "test_dataset = (test_dataset.shuffle(10)\n",
    "                .padded_batch(BATCH_SIZE, \n",
    "                              padded_shapes=({'log_adj_daily_returns': [TIMESTEPS,], \n",
    "                                              'docs': [TIMESTEPS, None]}, [])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-aab66ff54649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Preparing Inputs for Time Series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_log_returns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mdoc_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embedding_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;31m#ts_input = layers.Concatenate()([doc_features, num_features])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Programs/FinTech/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Programs/FinTech/env/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Programs/FinTech/env/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(instance, input_shape)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m       \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_tuples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Return shapes from `fn` as TensorShapes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Programs/FinTech/env/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0mshape_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_inputs_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mreduced_inputs_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m       \u001b[0mshape_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_inputs_shapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "feature_names = ['log_adj_daily_returns', 'docs_WFC', 'docs_JPM', 'docs_C', 'docs_BAC']\n",
    "\n",
    "# destroying already made graph nodes in the tensorflow backend\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "timesteps = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# document embedding Model\n",
    "\n",
    "    \n",
    "with open(os.path.join(path_to_data, 'vocab.json')) as f:\n",
    "    vocab = json.load(f)\n",
    "        \n",
    "#document = keras.Input(shape=(None,), name='document')\n",
    "#word_embedding = Word_Embedding(vocab, trainable=False)(document)\n",
    "#document_embedding = layers.LSTM(400)(word_embedding)\n",
    "#document_embedder = keras.Model(document, document_embedding, name='document_embedder')\n",
    "#print(document_embedder.summary())\n",
    "\n",
    "#keras.utils.plot_model(document_embedder, 'document_embedder.png', show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "# Inputs\n",
    "input_docs_WFC = keras.Input(shape=(timesteps, None), name='docs_WFC')\n",
    "#input_docs_JPM = keras.Input(shape=(timesteps, None), name='docs_JPM')\n",
    "#input_docs_BAC = keras.Input(shape=(timesteps, None), name='docs_BAC')\n",
    "#input_docs_C = keras.Input(shape=(timesteps, None), name='docs_C')\n",
    "input_log_returns = keras.Input(shape=(timesteps,), name='log_adj_daily_returns')\n",
    "\n",
    "# Splitting docs_WFC input into its individual timesteps\n",
    "timesteps_layer = [input_docs_WFC[:, t] for t in range(timesteps)]\n",
    "\n",
    "# Flattening Documents Dimension for each timestep (cause I don't know how to deal with the extra dimension for the LSTM)\n",
    "#flattened_timesteps_layer = [tf.reshape(timestep, [-1, tf.reduce_prod(tf.shape(timestep)[1:])]) for timestep in timesteps_layer]\n",
    "\n",
    "# Word Embedding Layer\n",
    "word_embedding = Word_Embedding(vocab, init='glove', trainable=True, mask_zero=True)\n",
    "word_embedding_layer = [word_embedding(timestep) for timestep in timesteps_layer]\n",
    "\n",
    "\n",
    "# Document Embedding Layer\n",
    "document_embedding = layers.LSTM(100)\n",
    "doc_embedding_layer = [document_embedding(timestep) for timestep in word_embedding_layer]\n",
    "\n",
    "# Preparing Inputs for Time Series\n",
    "num_features = tf.expand_dims(input_log_returns, -1)\n",
    "doc_features = tf.stack(doc_embedding_layer, axis=1)\n",
    "ts_input = layers.Concatenate()([doc_features, num_features])\n",
    "\n",
    "\n",
    "\n",
    "# Time Series Component\n",
    "time_series = layers.LSTM(100)(ts_input)\n",
    "\n",
    "# Output\n",
    "output = layers.Dense(1)(time_series)\n",
    "\n",
    "# Creating Model\n",
    "test_model = keras.Model({'docs_WFC': input_docs_WFC, 'log_adj_daily_returns': input_log_returns}, output, name='test_model')\n",
    "\n",
    "# Compiling Model\n",
    "test_model.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training Model\n",
    "#test_ds = tsds.batch(1).repeat()\n",
    "\n",
    "\n",
    "#test_model.fit_generator(test_ds, epochs=3, steps_per_epoch=100)\n",
    "keras.utils.plot_model(test_model, 'test.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tf.int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = tf.data.Dataset.range(10)\n",
    "d.element_spec.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def preprocess_pm(raw_df):\n",
    "    '''\n",
    "    Preprocessing raw_df into the shape raw_df should have been after coming out of the fetch process, as well as \n",
    "    normalizing the documents.\n",
    "    '''\n",
    "    \n",
    "    # Reshaping DataFrame\n",
    "    reshaped_df = reshape(raw_df)\n",
    "    \n",
    "    # Normalizing and updating documents    \n",
    "    def update_doclist(s):\n",
    "        doclist = json.loads(s)\n",
    "        updated_doclist = []\n",
    "        for docpath in doclist:\n",
    "            save_point = os.path.join(os.path.split(docpath)[0], 'normalized')\n",
    "            norm_docpath = normalize_save_document(docpath, save_point)\n",
    "            updated_doclist.append(norm_docpath)\n",
    "            \n",
    "        return json.dumps(updated_doclist)\n",
    "    \n",
    "    for t in tickers:\n",
    "        reshaped_df['_'.join(['docs', t])] = reshaped_df['_'.join(['docs', t])].map(update_doclist)\n",
    "        \n",
    "    # Preprocessing numerical data\n",
    "    reshaped_df['log_adj_close'] = np.log(reshaped_df['adjusted_close'])\n",
    "    reshaped_df['log_adj_daily_returns'] = reshaped_df['log_adj_close'] - reshaped_df['log_adj_close'].shift(-1)\n",
    "    reshaped_df.dropna(subset=['log_adj_daily_returns'], inplace=True)\n",
    "    \n",
    "    # Building vocabulary json file\n",
    "    path_to_vocab = os.path.join(path_to_data, 'vocab.json')\n",
    "    \n",
    "    def vocab_from_doclist(s):\n",
    "        \n",
    "        doclist = json.loads(s)\n",
    "        \n",
    "        for docpath in doclist:\n",
    "            with open(docpath, 'r') as f:\n",
    "                doc = f.read()\n",
    "            \n",
    "            build_vocab(doc)\n",
    "        \n",
    "        return json.dumps(doclist)\n",
    "    \n",
    "    for t in tickers:\n",
    "        reshaped_df['_'.join(['docs', t])].map(vocab_from_doclist)\n",
    "        \n",
    "    # Encoding documents based off of vocabulary json file\n",
    "    \n",
    "    \n",
    "    return reshaped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing tfrecords\n",
    "\n",
    "feature_names = ['log_adj_daily_returns', 'docs_WFC', 'docs_JPM', 'docs_C', 'docs_BAC']\n",
    "with open(os.path.join(path_to_data, 'vocab.json'), 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    '''Returns a bytes_list from a string / byte.'''\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    '''Returns a float_list from a float / double.'''\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    '''Returns an int64_list from a bool / enum / int / uint.'''\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(sample, feature_names):\n",
    "    \n",
    "    feature = {}\n",
    "    \n",
    "    for feature_name in feature_names:\n",
    "        # if feature is a number \n",
    "        if isinstance(sample[feature_name], float):\n",
    "            feature[feature_name] = tf.train.Feature(float_list=tf.train.FloatList(value=[sample[feature_name]]))\n",
    "        \n",
    "        # if feature is a doclist\n",
    "        elif isinstance(sample[feature_name], list):\n",
    "            lens = list(map(len, sample[feature_name]))\n",
    "            values = [word for doc in sample[feature_name] for word in doc]\n",
    "            feature[feature_name + '/vals'] = tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "            feature[feature_name + '/lens'] = tf.train.Feature(int64_list=tf.train.Int64List(value=lens))\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        \n",
    "\n",
    "\n",
    "def unpack_doclist(doclist_string):        \n",
    "    def load_encode_file(filename):\n",
    "        with open(filename, 'r') as f:\n",
    "            text = f.read()\n",
    "        return [vocab[word] for word in text.split()]\n",
    "    return list(map(load_encode_file, json.loads(doclist_string)))\n",
    "\n",
    "\n",
    "record_file = 'test.tfrecords'\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for i in range(300, -1, -1):\n",
    "        row = raw_df_fixed.iloc[i].copy(deep=True)\n",
    "        # Unpacking Text Features\n",
    "        row[['_'.join(['docs', t]) for t in tickers]] = row[['_'.join(['docs', t]) for t in tickers]].map(unpack_doclist)\n",
    "        # Serializing Example to disk\n",
    "        example = serialize_example(row, feature_names)\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing our data involves:\n",
    "1. Loading the dataset from the TFRecord file\n",
    "2. Splitting the dataset by stock ticker\n",
    "3. Reshaping each dataset to prepare it for training:\n",
    "    1. Windowing the dataset so each element produces a time series of features along with there corresponding label\n",
    "    2. Sampling the document feature for the document that will represent the specific window's document and cloning that document for each timestep in our defined window size\n",
    "    3. Filtering our dataset to include only elements with a document feature\n",
    "4. Concatenating the reshaped datasets together, and shuffling the dataset\n",
    "5. Splitting the dataset into train, validation, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the dataset from TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Dataset to TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sucessfully preprocessing our dataset we next write our dataset to a TFRecords file (https://www.tensorflow.org/tutorials/load_data/tfrecord) a binary file format that is read efficiently by the TensorFlow framework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Functions and Classes used to write TFRecord files\n",
    "\n",
    "def serialize_example(sample, feature_names):\n",
    "    '''\n",
    "    Maps dictionary :param sample: to a tf.train.Example object where the list feature_names determines which \n",
    "    subset of :param sample:'s keys are to be used. \n",
    "    \n",
    "    :param sample: dict, where the keys are the names of the features of the specific data sample, and the values \n",
    "                   are the values each feature takes on for the specific data sample\n",
    "    :param feature_names: list, of strings, a subset of sample.keys(), these are the features we will\n",
    "                          be considering for analysis\n",
    "    \n",
    "    ---> tf.train.Example, object representing the data sample\n",
    "    '''\n",
    "    \n",
    "    feature = {}\n",
    "    feature_description = {}\n",
    "    \n",
    "    for feature_name in feature_names:\n",
    "        # if feature is a float number \n",
    "        if isinstance(sample[feature_name], float):\n",
    "            feature[feature_name] = tf.train.Feature(float_list=tf.train.FloatList(value=[sample[feature_name]]))\n",
    "            feature_description[feature_name] = tf.io.FixedLenFeature([], tf.float32)\n",
    "        \n",
    "        # if feature is a list of documents\n",
    "        elif isinstance(sample[feature_name], list) and all(isinstance(word, int) for doc in sample[feature_name] for word in doc):\n",
    "            lens = list(map(len, sample[feature_name]))\n",
    "            values = [word for doc in sample[feature_name] for word in doc]\n",
    "            feature[feature_name + '/vals'] = tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "            feature[feature_name + '/lens'] = tf.train.Feature(int64_list=tf.train.Int64List(value=lens))\n",
    "            feature_description[feature_name + '/vals'] = tf.io.VarLenFeature(dtype=tf.int64)\n",
    "            feature_description[feature_name + '/lens'] = tf.io.VarLenFeature(dtype=tf.int64)\n",
    "        \n",
    "        # if feature is an integer number\n",
    "        elif isinstance(sample[feature_name], int):\n",
    "            feature[feature_name] = tf.train.Feature(int64_list=tf.train.Int64List(value=[sample[feature_name]]))\n",
    "            feature_description[feature_name] = tf.io.FixedLenFeature([], tf.int64)\n",
    "        \n",
    "        # Feature doesn't fit any of the tf example types\n",
    "        else:\n",
    "            raise ValueError('Value of Feature does not fit any of the tf.train.Feature serializable types')\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature)), feature_description\n",
    "\n",
    "\n",
    "def write_tfrecord(df, feature_names, filename='dataset.tfrecord'):\n",
    "    '''\n",
    "    Writes TFRecord file named :param filename: to dataset directory, and generates the corresponding \n",
    "    feature_description dictionary mapping a sample's feature name to the data type description of that\n",
    "    feature.\n",
    "    \n",
    "    :param df: pd.DataFrame, containing data and refrences to data that needs to be written to disk\n",
    "    :param feature_names, list of strings, a subset of the names of columns of df, that represents which subset\n",
    "                          of features from our preprocessed DataFrame that will be considered for modeling\n",
    "    :param filename: string, name of TFRecord file\n",
    "    \n",
    "    ---> dict, of names of features mapping to tf.io.VarLenFeature and tf.io.FixedLenFeature objects\n",
    "    '''\n",
    "    \n",
    "    def unpack_doclist(doclist_string):\n",
    "        '''\n",
    "        Takes a json format string that when loaded contains a list of paths to encoded document pickle files, and\n",
    "        returns a list of the objects loaded from these pickle files.\n",
    "        \n",
    "        :param doclist_string: string, json formated, contain a list of paths to encoded document pickle files\n",
    "        \n",
    "        ---> list, of encoded documents\n",
    "        '''\n",
    "        \n",
    "        def load_file(filename):\n",
    "            '''\n",
    "            Takes a path to a pickle file, and returns the loaded object.\n",
    "            \n",
    "            :param filename: string, path to pickle file\n",
    "            \n",
    "            ---> python object loaded file located at :param filename:\n",
    "            '''\n",
    "            \n",
    "            with open(filename, 'rb') as f:\n",
    "                doc = pickle.load(f)\n",
    "            return doc\n",
    "        \n",
    "        return list(map(load_file, json.loads(doclist_string)))\n",
    "    \n",
    "    with tf.io.TFRecordWriter(os.path.join(path_to_data, filename)) as writer:\n",
    "        print('Writing TFRecord file to: {}'.format(os.path.join(path_to_data, filename)))\n",
    "        for i in range(len(df)-1, -1, -1):\n",
    "            row = df.iloc[i].copy(deep=True)\n",
    "            # Unpacking Text Features\n",
    "            row[['_'.join(['docs', t]) for t in tickers]] = row[['_'.join(['docs', t]) for t in tickers]].map(unpack_doclist)\n",
    "            # Serializing Example to disk\n",
    "            example, feature_description = serialize_example(row, feature_names)\n",
    "            writer.write(example.SerializeToString())\n",
    "    print('Finished writing TFRecord file.')\n",
    "\n",
    "    return feature_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features we will be considering for analysis are: log_adj_daily_returns_WFC, log_adj_daily_returns_JPM, log_adj_daily_returns_BAC, log_adj_daily_returns_C, docs_WFC, docs_JPM, docs_BAC, docs_C\n",
      "\n",
      "Writing TFRecord file to: /media/Data/Programs/FinTech/data/dataset.tfrecord\n",
      "Finished writing TFRecord file.\n"
     ]
    }
   ],
   "source": [
    "# Writing TFRecord file\n",
    "\n",
    "# Defining the subset of features from our preprocessed DataFrame that we will be using for modeling\n",
    "feature_names = ['_'.join([feature, t]) for feature in ['log_adj_daily_returns', 'docs'] for t in tickers]\n",
    "print('The features we will be considering for analysis are: {}'.format(', '.join(feature_names)))\n",
    "print()\n",
    "# Loading the preprocessed DataFrame from disk if it has not been instantiated\n",
    "try:\n",
    "    preprocessed_df\n",
    "except:\n",
    "    preprocessed_df = pd.read_csv(os.path.join(path_to_data, 'preprocessed.csv'), parse_dates=['timestamp'])\n",
    "# Writing the TFRecord file\n",
    "feature_description = write_tfrecord(preprocessed_df, feature_names)\n",
    "# Writing the TFRecord's feature_description object to disk\n",
    "with open(os.path.join(path_to_data, 'dataset_feature_description.pickle'), 'wb') as f:\n",
    "    pickle.dump(feature_description, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we will be using only the bare minimum amount of features for modeling ie: our text features listed in the doc columns and the feature we are trying to predict in time series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
