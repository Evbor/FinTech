{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Predicting Stock Yields using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- update text portions of notebook for intro price EDA\n",
    "- make notebook able to updated dfs.\n",
    "- get rid of hardcoded feature names in modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning NLP techniques to map raw text to dense vector representations have had some surprising success in the world of computer natural language processing compared to classical means of encoding text. In this project we will attempt to leverage some of these techniques to help assist us in time series analysis on stock yields. The company we will choose to investigate is Wells Fargo (stock ticker: WFC), and the text data we will be leveraging are the SEC forms of Wells Fargo along with its competitors: JPMorgan Chase, Bank of America, and Citigroup. Specifically the 8-K form. The 8-K form was chosen because it tends to be the more text rich SEC document when compared to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Notebook Global Variables\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# Project Paths\n",
    "project_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "path_to_data = os.path.join(project_dir, 'data')\n",
    "path_to_docs = os.path.join(path_to_data, 'documents')\n",
    "# Company Stock Ticker and CIK number\n",
    "ticker = 'WFC'\n",
    "competitors = ['JPM', 'BAC', 'C']\n",
    "tickers = [ticker] + competitors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sucessfully preprocessing our dataset we next write our dataset to a TFRecords file (https://www.tensorflow.org/tutorials/load_data/tfrecord) a binary file format that is read efficiently by the TensorFlow framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook we will be using only the bare minimum amount of features for modeling ie: our text features listed in the doc columns and the feature we are trying to predict in time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to discover whether we can construct a deep learning model that correlates (predicts) stock price data with unstructured text data found in company 8-K forms. Specifically for our first models we will try to correlate the logarithmic returns of a stock with the previous logarithmic returns of the stock in a given window of time, along with the text data stored in the companies 8-K forms that were released in the given window of time. If there are multiple 8-K documents released in the given window of time then our text data fed to the model will be derived from a single 8-K document uniformly sampled from the 8-K documents released in the given window of time. This window size (in days) along with architecture of the model will be the hyperparameters that can be tuned when experiment with different model designs. Modeling will consist of two phases: Preparing Data, and Evaluating Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n",
      "Visible GPUs: []\n"
     ]
    }
   ],
   "source": [
    "# Importing Libraries and Configuring virtual GPU\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from fractions import Fraction\n",
    "from functools import reduce\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "visible_gpus = tf.config.experimental.get_visible_devices('GPU')\n",
    "print('GPUs: {}'.format(gpus))\n",
    "print('Visible GPUs: {}'.format(visible_gpus))\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5000)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Setting Random Seed\n",
    "tf.random.set_seed(20)\n",
    " \n",
    "# Model Hyperparameters    \n",
    "## Model Hyperparameters that require reshaping Dataset\n",
    "TIMESTEPS = 8\n",
    "BATCH_SIZE = 10\n",
    "with open(os.path.join(path_to_data, 'vocab.json'), 'r') as f:\n",
    "    vocab = json.load(f)\n",
    "## Model 1 Hyperparameters\n",
    "DOC_EMBEDDING_UNITS = 1000\n",
    "#TS_LAYER_1_UNITS = 700\n",
    "TS_LAYER_2_UNITS = 50\n",
    "TS_LAYER_3_UNITS = 50\n",
    "OPTIMIZER = keras.optimizers.Adam\n",
    "LOSS = keras.losses.BinaryCrossentropy()\n",
    "METRICS = ['accuracy']\n",
    "LEARNING_RATE=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing our data involves:\n",
    "1. Loading the dataset from the TFRecord file\n",
    "2. Splitting the dataset by stock ticker\n",
    "3. Reshaping each dataset to prepare it for training:\n",
    "    1. Windowing the dataset so each element produces a time series of features along with there corresponding label\n",
    "    2. Sampling the document feature for the document that will represent the specific window's document and cloning that document for each timestep in our defined window size\n",
    "    3. Filtering our dataset to include only elements with a document feature\n",
    "4. Concatenating the reshaped datasets together, and shuffling the dataset\n",
    "5. Splitting the dataset into train, validation, and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading the dataset from TFRecord file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions and classes used to load the dataset from its TFRecord file\n",
    "\n",
    "def parse_example(example_proto, feature_description):\n",
    "    '''\n",
    "    Parses example proto from\n",
    "    \n",
    "    :param example_proto: \n",
    "    :param feature_description: \n",
    "    '''\n",
    "    \n",
    "    # Parse the input tf.Example proto using the dictionary above.\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    \n",
    "    # Reconstructing Ragged Tensors from Example\n",
    "    for t in tickers:\n",
    "        example['_'.join(['docs', t])] = tf.RaggedTensor.from_row_lengths(example['docs_{}/vals'.format(t)].values,\n",
    "                                                           row_lengths=example['docs_{}/lens'.format(t)].values)\n",
    "\n",
    "    # Deleting Redundant Keys\n",
    "    for t in tickers:\n",
    "        del example['docs_{}/vals'.format(t)]\n",
    "        del example['docs_{}/lens'.format(t)]\n",
    "        \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Dataset\n",
    "\n",
    "# Loading the raw dataset from the TFRecord file\n",
    "dataset = tf.data.TFRecordDataset(os.path.join(path_to_data, 'dataset.tfrecord'))\n",
    "# Loading the dataset's feature_description\n",
    "with open(os.path.join(path_to_data, 'dataset_feature_description.pickle'), 'rb') as f:\n",
    "    feature_description = pickle.load(f)\n",
    "# Decoding the raw dataset using the dataset's feature_description\n",
    "dataset = dataset.map(lambda example_proto: parse_example(example_proto, feature_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spliting the dataset by stock ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(example, features, ticker):\n",
    "    return {feature_name: example['_'.join([feature_name, ticker])] for feature_name in features}\n",
    "\n",
    "datasets = [dataset.map(lambda ex: split(ex, ['log_adj_daily_returns', 'docs'], t)) for t in tickers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Reshaping datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions and classes used to reshape datasets\n",
    "\n",
    "def make_window_dataset(ds, window_size, shift=1, stride=1):\n",
    "    \n",
    "    windows = ds.window(window_size, shift=shift, stride=stride)\n",
    "    \n",
    "    feature_datasets = {key: windows.flat_map(lambda x: x[key].batch(window_size, drop_remainder=True))\n",
    "                        for key in windows.element_spec.keys()}\n",
    "    \n",
    "    return tf.data.Dataset.zip(feature_datasets)\n",
    "\n",
    "def extract_labels(timeslice, label_features):\n",
    "    labels = {}\n",
    "    \n",
    "    for feature_key in timeslice.keys():\n",
    "        feature_timeslice = timeslice[feature_key]\n",
    "        if feature_key in label_features:\n",
    "            labels[feature_key] = feature_timeslice[-1]\n",
    "        timeslice[feature_key] = feature_timeslice[:-1]\n",
    "        \n",
    "    return (timeslice, labels)\n",
    "\n",
    "\n",
    "def to_time_series(ds, label_features, window_size, steps_to_pred=1, num_of_preds=1):\n",
    "    \n",
    "    # making full time series Dataset object (features + labels)\n",
    "    full_ts_ds = make_window_dataset(ds, window_size=window_size+1)\n",
    "    \n",
    "    # mapping dataset to Dataset where each el is: (features: dict, labels)\n",
    "    ts_ds = full_ts_ds.map(lambda s: extract_labels(s, label_features))\n",
    "    \n",
    "    return ts_ds\n",
    "\n",
    "def sample_documents(sample):\n",
    "    # Extracting all documents in the sample\n",
    "    docs_in_sample = sample.values\n",
    "    # Sampling a random document from all the documents in the sample\n",
    "    if docs_in_sample.nrows() != 0:\n",
    "        i = tf.random.uniform([1], maxval=docs_in_sample.nrows(), dtype=tf.int64)[0]\n",
    "        sample_doc = docs_in_sample[i]\n",
    "    else:\n",
    "        sample_doc = tf.constant([], dtype=tf.int64)\n",
    "        \n",
    "    return sample_doc\n",
    "\n",
    "def select_doc(features, labels):\n",
    "    \n",
    "    for fname in features.keys():\n",
    "        feature = features[fname]\n",
    "        timesteps = feature.shape[0]\n",
    "        # Feature is a doc feature\n",
    "        if isinstance(feature, tf.RaggedTensor):\n",
    "            doc = sample_documents(feature)\n",
    "            feature = tf.stack([doc for day in range(timesteps)])\n",
    "            features[fname] = feature\n",
    "        \n",
    "    return (features, *list(labels.values()))\n",
    "\n",
    "def filter_fn(f, l):\n",
    "    shape = tf.shape(f['docs'])[1]\n",
    "    return tf.math.not_equal(shape, 0)\n",
    "\n",
    "def reshape(dataset, window_size, label_name):\n",
    "    # Converting to time series\n",
    "    ds = to_time_series(dataset, label_name, window_size=window_size)\n",
    "    # Selecting document features\n",
    "    ds = ds.map(select_doc)\n",
    "    # Filtering out elements without a document feature\n",
    "    ds = ds.filter(filter_fn)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping Datasets\n",
    "reshaped_datasets = list(map(lambda d: reshape(d, TIMESTEPS, 'log_adj_daily_returns'), datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "4. Concatenating datasets, and shuffling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = reduce(lambda a, b: a.concatenate(b), reshaped_datasets).shuffle(1000, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Splitting dataset into train, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Functions and Classes for splitting datasets into train, validation, and test datasets\n",
    "\n",
    "def k_folds(dataset, k):\n",
    "    '''\n",
    "    Splits :param dataset: into :param k: number of equally sized (or close to equally sized) components.\n",
    "    \n",
    "    :param dataset: tf.data.Dataset, dataset to split into k folds\n",
    "    :param k: int, number of folds to split :param dataset: into\n",
    "    \n",
    "    ---> list, of tf.data.Dataset objets\n",
    "    '''\n",
    "    return [dataset.shard(k, i) for i in range(k)]\n",
    "\n",
    "def train_test_split(dataset, train_size):\n",
    "    '''\n",
    "    Splits :param dataset: into\n",
    "    \n",
    "    :param dataset: tf.data.Dataset, to split into train and test datasets\n",
    "    :param train_size: float between 0 and 1, proportion of :param dataset: to put into train dataset\n",
    "    \n",
    "    ---> (tf.data.Dataset, tf.data.Dataset), representing train, test datasets\n",
    "    '''\n",
    "    train_size = Fraction(train_size).limit_denominator()\n",
    "    x, k = train_size.numerator, train_size.denominator\n",
    "    folds = k_folds(dataset, k)\n",
    "    train = reduce(lambda a, b: a.concatenate(b), folds[:x])\n",
    "    test = reduce(lambda a, b: a.concatenate(b), folds[x:])\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our models we will reserve 60% of the dataset for training, 20% for validation, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting our dataset into train, validation, test datasets\n",
    "\n",
    "# Creating datasets\n",
    "train_dataset, test_val_dataset = train_test_split(dataset, train_size=0.6)\n",
    "val_dataset, test_dataset = train_test_split(test_val_dataset, train_size=0.5)\n",
    "\n",
    "# Prepping datasets for modeling\n",
    "train_dataset = (train_dataset.shuffle(10)\n",
    "                 .padded_batch(batch_size, \n",
    "                               padded_shapes=({'log_adj_daily_returns': [TIMESTEPS,], \n",
    "                                               'docs': [TIMESTEPS, None]}, [])))\n",
    "val_dataset = (val_dataset.shuffle(10)\n",
    "               .padded_batch(batch_size, \n",
    "                             padded_shapes=({'log_adj_daily_returns': [TIMESTEPS,], \n",
    "                                             'docs': [TIMESTEPS, None]}, [])))\n",
    "test_dataset = (test_dataset.shuffle(10)\n",
    "                .padded_batch(batch_size, \n",
    "                              padded_shapes=({'log_adj_daily_returns': [TIMESTEPS,], \n",
    "                                              'docs': [TIMESTEPS, None]}, [])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions and classes used to construct model layers\n",
    "\n",
    "def embedding_matrix(vocab, init):\n",
    "    '''\n",
    "    Constructs the embedding matrix for specific init type for a pre initialized word embedding layer.\n",
    "    \n",
    "    :param vocab: dict, a mapping between keys of words, and values of unique integer identifiers for each word\n",
    "    :param init: string, initialization type currently we only support glove initialization\n",
    "    \n",
    "    ---> numpy array of size (vocab length, embedding dimension) mapping each word encoding to a vector\n",
    "    '''\n",
    "    \n",
    "    if init == 'glove':\n",
    "        glove_dir = 'glove'\n",
    "        \n",
    "        try:\n",
    "            with open(os.path.join(glove_dir, 'current_embedding.pickle'), 'rb') as f:\n",
    "                embedding_m = pickle.load(f)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            # Building word to vector map\n",
    "            word_embeddings = {}\n",
    "            with open(os.path.join(glove_dir, 'glove.840B.300d.txt')) as f:\n",
    "                for line in f:\n",
    "                    tokens = line.split(' ')\n",
    "                    word = tokens[0]\n",
    "                    embedding = np.asarray(tokens[1:], dtype='float32')\n",
    "                    # Needs to check if dim is changing\n",
    "                    assert len(embedding) == 300\n",
    "                    word_embeddings[word] = embedding\n",
    "            # Building embedding matrix\n",
    "            EMBEDDING_DIM = len(next(iter(word_embeddings.values())))\n",
    "            embedding_m = np.zeros((len(vocab) + 1, EMBEDDING_DIM))\n",
    "            for word, i in vocab.items():\n",
    "                embedding_vector = word_embeddings.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_m[i] = embedding_vector\n",
    "            # Saving embedding matrix\n",
    "            with open(os.path.join(glove_dir, 'current_embedding.pickle'), 'wb') as f:\n",
    "                pickle.dump(embedding_m, f)\n",
    "                \n",
    "    else:\n",
    "        raise ValueError('init type not supported, init must be equal to \"glove\"')\n",
    "\n",
    "    return embedding_m\n",
    "\n",
    "def Word_Embedding(vocab, init, \n",
    "                   embeddings_initializer='uniform', embeddings_regularizer=None, \n",
    "                   activity_regularizer=None, embeddings_constraint=None, \n",
    "                   mask_zero=False, input_length=None, **kwargs):\n",
    "    \n",
    "    '''\n",
    "    Creates a keras embedding layer specifically designed to embed the words specified in :param vocab:\n",
    "    \n",
    "    :param vocab: dict, representing the mapping between the words in corpus (keys) and their unique integer\n",
    "                  encodings\n",
    "    :param init: string or int, tells the layer how to initialize its embeddings. If of type int, then\n",
    "                 it tells the layer to initialize its word embeddings with an embedding dimension of :param init:.\n",
    "                 If of type string, then :param init: specifies the type of pretrained word embeddings we will be \n",
    "                 initializing the embedding layer with\n",
    "    \n",
    "    ---> tf.keras.layers.Embedding\n",
    "    '''\n",
    "    \n",
    "    if isinstance(init, str):\n",
    "        current_embedding_matrix = embedding_matrix(vocab, init)\n",
    "        emb_layer = layers.Embedding(current_embedding_matrix.shape[0], current_embedding_matrix.shape[1],\n",
    "                                     weights=[current_embedding_matrix], mask_zero=mask_zero,\n",
    "                                     input_length=None, **kwargs)\n",
    "        \n",
    "    elif isinstance(init, int):\n",
    "        emb_layer = layers.Embedding(len(vocab) + 1, output_dim=init, \n",
    "                                     embeddings_initializer=embeddings_initializer, embeddings_regularizer=embeddings_regularizer, \n",
    "                                     activity_regularizer=activity_regularizer, embeddings_constraint=embeddings_constraint, \n",
    "                                     mask_zero=mask_zero, input_length=input_length, **kwargs)\n",
    "    else:\n",
    "        raise ValueError('init type not supported')\n",
    "        \n",
    "    return emb_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions and classes used to construct models\n",
    "\n",
    "def model_1(timesteps, vocab, doc_embedding_size, ts_layer_1_size, ts_layer_2_size, ts_layer_3_size,\n",
    "            optimizer, learning_rate, loss, metrics):\n",
    "    '''\n",
    "    Constructs a model with the architecture of \n",
    "    '''\n",
    "    \n",
    "    # inputs\n",
    "    input_docs = keras.Input(shape=(timesteps, None), name='docs', dtype=tf.int64)\n",
    "    input_log_returns = keras.Input(shape=(timesteps,), name='log_adj_daily_returns', dtype=tf.float32)\n",
    "    \n",
    "    # Preparing Features for Time Series Analysis\n",
    "    num_features = tf.expand_dims(input_log_returns, -1)\n",
    "    ts_input = num_featres\n",
    "    \n",
    "    # Building Time Series Layer\n",
    "    ts_layer_2 = layers.LSTM(ts_layer_2_size, activation='relu', return_sequences=True)(ts_input)\n",
    "    ts_layer_3 = layers.LSTM(ts_layer_3_size, activation='relu')(ts_layer_2)\n",
    "    \n",
    "    # Building Output Layer\n",
    "    output = layers.Dense(1, activation='sigmoid')(ts_layer_3)\n",
    "    \n",
    "    # Building Model\n",
    "    model = keras.Model([input_docs, input_log_returns], output, name='model_1')\n",
    "    \n",
    "    if learning_rate == None:\n",
    "        opt = optimizer()\n",
    "    else:\n",
    "        opt = optimizer(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=opt, loss=loss, metrics=metrics)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "log_adj_daily_returns (InputLay [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 8, 1)]       0           log_adj_daily_returns[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 8, 50)        10400       tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 50)           20200       lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "docs (InputLayer)               [(None, 8, None)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            51          lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 30,651\n",
      "Trainable params: 30,651\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "758/758 [==============================] - 121s 160ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "758/758 [==============================] - 101s 133ms/step - loss: 0.6932 - accuracy: 0.4921 - val_loss: 0.6932 - val_accuracy: 0.4995\n",
      "Epoch 3/30\n",
      "758/758 [==============================] - 118s 156ms/step - loss: 0.6933 - accuracy: 0.5009 - val_loss: 0.6933 - val_accuracy: 0.4995\n",
      "Epoch 4/30\n",
      "758/758 [==============================] - 110s 145ms/step - loss: 0.6933 - accuracy: 0.4913 - val_loss: 0.6932 - val_accuracy: 0.4995\n",
      "Epoch 5/30\n",
      "758/758 [==============================] - 106s 140ms/step - loss: 0.6933 - accuracy: 0.4934 - val_loss: 0.6932 - val_accuracy: 0.4997\n",
      "Epoch 6/30\n",
      "758/758 [==============================] - 105s 139ms/step - loss: 0.6933 - accuracy: 0.4927 - val_loss: 0.6931 - val_accuracy: 0.5001\n",
      "Epoch 7/30\n",
      "758/758 [==============================] - 117s 154ms/step - loss: 0.6946 - accuracy: 0.4930 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "Epoch 8/30\n",
      "758/758 [==============================] - 125s 165ms/step - loss: 0.6931 - accuracy: 0.4931 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 9/30\n",
      "758/758 [==============================] - 110s 145ms/step - loss: 0.6930 - accuracy: 0.4934 - val_loss: 0.6929 - val_accuracy: 0.4999\n",
      "Epoch 10/30\n",
      "758/758 [==============================] - 100s 131ms/step - loss: 0.6930 - accuracy: 0.4925 - val_loss: 0.6929 - val_accuracy: 0.4999\n",
      "Epoch 11/30\n",
      "758/758 [==============================] - 71s 94ms/step - loss: 0.6929 - accuracy: 0.4942 - val_loss: 0.6929 - val_accuracy: 0.5003\n",
      "Epoch 12/30\n",
      "758/758 [==============================] - 74s 97ms/step - loss: 0.6930 - accuracy: 0.4918 - val_loss: 0.6929 - val_accuracy: 0.5001\n",
      "Epoch 13/30\n",
      "758/758 [==============================] - 72s 95ms/step - loss: 0.6930 - accuracy: 0.4934 - val_loss: 0.6929 - val_accuracy: 0.5003\n",
      "Epoch 14/30\n",
      "758/758 [==============================] - 71s 94ms/step - loss: 0.6948 - accuracy: 0.4933 - val_loss: 0.6931 - val_accuracy: 0.4999\n",
      "Epoch 15/30\n",
      "758/758 [==============================] - 71s 94ms/step - loss: 0.6932 - accuracy: 0.4914 - val_loss: 0.6930 - val_accuracy: 0.5000\n",
      "Epoch 16/30\n",
      "758/758 [==============================] - 70s 93ms/step - loss: 0.6931 - accuracy: 0.4970 - val_loss: 0.6929 - val_accuracy: 0.4999\n",
      "Epoch 17/30\n",
      "758/758 [==============================] - 70s 93ms/step - loss: 0.6930 - accuracy: 0.4922 - val_loss: 0.6928 - val_accuracy: 0.4997\n",
      "Epoch 18/30\n",
      "758/758 [==============================] - 72s 95ms/step - loss: 0.6929 - accuracy: 0.4925 - val_loss: 0.6928 - val_accuracy: 0.5001\n",
      "Epoch 19/30\n",
      "758/758 [==============================] - 72s 96ms/step - loss: 0.6929 - accuracy: 0.4934 - val_loss: 0.6928 - val_accuracy: 0.5003\n",
      "Epoch 20/30\n",
      "758/758 [==============================] - 75s 98ms/step - loss: 0.6950 - accuracy: 0.4922 - val_loss: 0.6931 - val_accuracy: 0.4997\n",
      "Epoch 21/30\n",
      "758/758 [==============================] - 74s 98ms/step - loss: 0.6932 - accuracy: 0.4945 - val_loss: 0.6931 - val_accuracy: 0.4999\n",
      "Epoch 22/30\n",
      "758/758 [==============================] - 70s 93ms/step - loss: 0.6931 - accuracy: 0.4959 - val_loss: 0.6930 - val_accuracy: 0.4999\n",
      "Epoch 23/30\n",
      "758/758 [==============================] - 72s 95ms/step - loss: 0.6931 - accuracy: 0.4962 - val_loss: 0.6930 - val_accuracy: 0.5001\n",
      "Epoch 24/30\n",
      "758/758 [==============================] - 72s 94ms/step - loss: 0.6930 - accuracy: 0.4942 - val_loss: 0.6929 - val_accuracy: 0.5003\n",
      "Epoch 25/30\n",
      "758/758 [==============================] - 72s 94ms/step - loss: 0.6929 - accuracy: 0.4955 - val_loss: 0.6928 - val_accuracy: 0.5000\n",
      "Epoch 26/30\n",
      "758/758 [==============================] - 70s 93ms/step - loss: 0.6930 - accuracy: 0.4959 - val_loss: 0.6929 - val_accuracy: 0.5004\n",
      "Epoch 27/30\n",
      "758/758 [==============================] - 72s 95ms/step - loss: 0.6930 - accuracy: 0.4930 - val_loss: 0.6929 - val_accuracy: 0.5003\n",
      "Epoch 28/30\n",
      "758/758 [==============================] - 70s 92ms/step - loss: 0.6943 - accuracy: 0.4945 - val_loss: 0.6931 - val_accuracy: 0.4999\n",
      "Epoch 29/30\n",
      "758/758 [==============================] - 73s 96ms/step - loss: 0.6931 - accuracy: 0.4968 - val_loss: 0.6930 - val_accuracy: 0.4999\n",
      "Epoch 30/30\n",
      "758/758 [==============================] - 72s 95ms/step - loss: 0.6930 - accuracy: 0.4975 - val_loss: 0.6930 - val_accuracy: 0.5001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfa70e3668>"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Model 1 with the current hyperparameters\n",
    "tf.keras.backend.clear_session()\n",
    "model = model_1(timesteps=TIMESTEPS, vocab=vocab, doc_embedding_size=DOC_EMBEDDING_UNITS,\n",
    "                ts_layer_1_size=None, ts_layer_2_size=TS_LAYER_2_UNITS, ts_layer_3_size=TS_LAYER_3_UNITS,\n",
    "                optimizer=OPTIMIZER, learning_rate=LEARNING_RATE, loss=LOSS, metrics=METRICS)\n",
    "\n",
    "print(model.summary())\n",
    "# Converting data to catagorical data\n",
    "def to_categorical(f, l):\n",
    "    if l > 0:\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 0\n",
    "    return (f, c)\n",
    "\n",
    "train_dataset = train_dataset.unbatch().map(to_categorical).batch(10)\n",
    "model.fit(train_dataset, epochs=30, validation_data=train_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
